{"cells":[{"cell_type":"markdown","id":"0b0ade82","metadata":{"id":"0b0ade82"},"source":["# Taller Pr치ctico: Fundamentos de LangChain, Agentes e Inteligencia Artificial\n","\n","## Bienvenida\n","\n","Este taller te guiar치 paso a paso en el uso de LangChain, una de las herramientas m치s poderosas para construir aplicaciones con modelos de lenguaje grandes (LLMs). A trav칠s de ejemplos pr치cticos centrados en an치lisis meteorol칩gico, aprender치s desde los conceptos b치sicos hasta la creaci칩n de agentes aut칩nomos.\n","\n","## Objetivos Educativos\n","\n","- Comprender la arquitectura y filosof칤a de LangChain 1.0+\n","- Dominar los diferentes tipos de mensajes y prompts\n","- Aprender a crear y usar plantillas de prompts (PromptTemplate, ChatPromptTemplate, FewShotChatMessagePromptTemplate)\n","- Construir cadenas (chains) para procesar informaci칩n de forma secuencial\n","- Implementar parsers para procesar y estructurar las salidas de los modelos\n","- Utilizar memoria conversacional en aplicaciones de IA\n","- Serializar y cargar prompts para reutilizaci칩n\n","- Crear herramientas personalizadas para agentes\n","- Dise침ar y ejecutar agentes aut칩nomos\n","- Aplicar mejores pr치cticas en el desarrollo de aplicaciones con LLMs\n","\n","## Resultados Esperados\n","\n","Al finalizar este taller, ser치s capaz de:\n","\n","- Construir aplicaciones conversacionales con memoria persistente\n","- Crear agentes que utilicen herramientas externas para resolver problemas complejos\n","- Dise침ar prompts efectivos y reutilizables\n","- Procesar y validar las salidas de los modelos de lenguaje\n","\n","\n","## Prerrequisitos\n","\n","- Conocimientos b치sicos de Python\n","- Cuenta de Google (para usar Google Colab)\n","- Disposici칩n para experimentar y aprender\n","\n","---"]},{"cell_type":"markdown","id":"b78927b5","metadata":{"id":"b78927b5"},"source":["## 1. Instalaci칩n y Configuraci칩n Inicial\n","\n","Antes de comenzar, necesitamos instalar las bibliotecas necesarias. LangChain es un framework modular, por lo que instalaremos los componentes espec칤ficos que utilizaremos.\n","\n","### 쯈u칠 es LangChain?\n","\n","LangChain es un framework dise침ado para simplificar la creaci칩n de aplicaciones con modelos de lenguaje grandes. Proporciona abstracciones y herramientas que permiten:\n","\n","- Conectar LLMs con fuentes de datos externas\n","- Mantener contexto conversacional\n","- Crear cadenas de procesamiento complejas\n","- Construir agentes que toman decisiones aut칩nomas\n","\n","En este taller usaremos Google Gemini, que ofrece una API gratuita para desarrollo en Google Colab."]},{"cell_type":"code","execution_count":null,"id":"3e6b9db6","metadata":{"id":"3e6b9db6"},"outputs":[],"source":["# Instalaci칩n de dependencias necesarias\n","# langchain-core: componentes b치sicos de LangChain\n","# langchain-google-genai: integraci칩n con modelos de Google Gemini\n","# langchain: framework principal\n","\n","!pip install -q langchain langchain-core langchain-google-genai langchain-community"]},{"cell_type":"markdown","id":"5df6ea87","metadata":{"id":"5df6ea87"},"source":["### Configuraci칩n de la API de Google Gemini\n","\n","Google Colab proporciona acceso gratuito a trav칠s de `google.colab.userdata`. Para obtener tu API key:\n","\n","1. Ve a [Google AI Studio](https://aistudio.google.com/app/apikey)\n","2. Crea una API key\n","3. En Colab, ve al 칤cono de llave (游댐) en el panel izquierdo\n","4. Agrega un secreto llamado `GOOGLE_API_KEY` con tu API key"]},{"cell_type":"code","execution_count":null,"id":"a935d57f","metadata":{"id":"a935d57f"},"outputs":[],"source":["# Importamos las librer칤as necesarias\n","import os\n","from google.colab import userdata\n","\n","# Configuramos la API key de forma segura\n","# En Colab: Click en la llave (游댐) en el panel izquierdo y agrega tu API key con el nombre \"GOOGLE_API_KEY\"\n","os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n","\n","print(\"Configuraci칩n completada correctamente\")"]},{"cell_type":"markdown","id":"fe6e39e5","metadata":{"id":"fe6e39e5"},"source":["---\n","\n","## 2. Primer Contacto con LangChain y Gemini\n","\n","Comencemos con la interacci칩n m치s b치sica: enviar un mensaje a un modelo de lenguaje y recibir una respuesta.\n","\n","### Concepto: Chat Model\n","\n","Un Chat Model es una abstracci칩n de LangChain que representa un modelo de lenguaje optimizado para conversaciones. A diferencia de los modelos de completado de texto, los chat models entienden el contexto de una conversaci칩n con mensajes de diferentes roles (usuario, asistente, sistema).\n","\n","### Tipos de Mensajes\n","\n","LangChain define varios tipos de mensajes:\n","\n","- **SystemMessage**: Instrucciones iniciales que gu칤an el comportamiento del modelo\n","- **HumanMessage**: Mensajes del usuario\n","- **AIMessage**: Respuestas del modelo\n","- **FunctionMessage** / **ToolMessage**: Mensajes relacionados con llamadas a funciones\n","- **ChatMessage**: Mensaje gen칠rico con rol personalizado"]},{"cell_type":"code","execution_count":null,"id":"60413172","metadata":{"id":"60413172"},"outputs":[],"source":["# Importamos el modelo de chat de Google Gemini\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","\n","# Inicializamos el modelo\n","# Gemini 2.5 Flash es un modelo r치pido y eficiente para la mayor칤a de tareas\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-2.5-flash-lite\",\n","    temperature=0.7,  # Controla la creatividad (0 = determinista, 1 = muy creativo)\n",")\n","\n","# Creamos un mensaje del usuario\n","mensaje = HumanMessage(content=\"쮺u치l es la temperatura ideal para el crecimiento de tomates?\")\n","\n","# Invocamos el modelo\n","respuesta = llm.invoke([mensaje])\n","\n","print(\"Tipo de respuesta:\", type(respuesta))\n","print(\"\\nContenido:\", respuesta.content)"]},{"cell_type":"markdown","id":"be796be1","metadata":{"id":"be796be1"},"source":["### Ejemplo: Uso de SystemMessage\n","\n","Los mensajes de sistema son fundamentales para definir el comportamiento y personalidad del modelo. Veamos c칩mo influyen en las respuestas."]},{"cell_type":"code","execution_count":null,"id":"3b4a9d70","metadata":{"id":"3b4a9d70"},"outputs":[],"source":["# Ejemplo con mensaje de sistema\n","mensajes = [\n","    SystemMessage(content=\"\"\"Eres un meteor칩logo experto especializado en an치lisis clim치tico.\n","    Responde de forma t칠cnica pero comprensible, siempre incluyendo datos cient칤ficos.\"\"\"),\n","    HumanMessage(content=\"쯇or qu칠 llueve m치s en algunas regiones que en otras?\")\n","]\n","\n","respuesta = llm.invoke(mensajes)\n","print(\"Respuesta del meteor칩logo experto:\")\n","print(respuesta.content)\n","print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Ahora sin mensaje de sistema\n","mensajes_simple = [\n","    HumanMessage(content=\"쯇or qu칠 llueve m치s en algunas regiones que en otras?\")\n","]\n","\n","respuesta_simple = llm.invoke(mensajes_simple)\n","print(\"Respuesta sin contexto de sistema:\")\n","print(respuesta_simple.content)"]},{"cell_type":"markdown","id":"6385dc39","metadata":{"id":"6385dc39"},"source":["### Reflexi칩n\n","\n","Observa c칩mo el SystemMessage modifica el tono y profundidad de la respuesta. 쯅otaste diferencias en el estilo t칠cnico entre ambas respuestas?\n","\n","### Ejercicio Pr치ctico 1\n","\n","Modifica el SystemMessage del bloque anterior para que el modelo responda como un profesor de primaria explicando conceptos clim치ticos a ni침os de 8 a침os. Ejecuta nuevamente y compara las respuestas."]},{"cell_type":"markdown","id":"642f8f6b","metadata":{"id":"642f8f6b"},"source":["---\n","\n","## 3. Plantillas de Prompts (Prompt Templates)\n","\n","Las plantillas permiten crear prompts reutilizables con variables din치micas. Esto es fundamental para mantener consistencia y facilitar el mantenimiento del c칩digo.\n","\n","### 3.1 PromptTemplate (Plantillas Simples)\n","\n","La clase PromptTemplate es la forma m치s b치sica de crear plantillas. Se usa principalmente para modelos de completado de texto, pero es importante entenderla como fundamento."]},{"cell_type":"code","execution_count":null,"id":"046ea273","metadata":{"id":"046ea273"},"outputs":[],"source":["from langchain_core.prompts import PromptTemplate\n","\n","# Crear una plantilla simple con variables\n","# Las variables se definen con llaves: {nombre_variable}\n","template = \"\"\"Eres un analista meteorol칩gico.\n","\n","Analiza las siguientes condiciones clim치ticas:\n","Ciudad: {ciudad}\n","Temperatura actual: {temperatura}춿C\n","Humedad: {humedad}%\n","Presi칩n atmosf칠rica: {presion} hPa\n","\n","Proporciona un an치lisis breve y profesional de estas condiciones.\"\"\"\n","\n","# Creamos la plantilla\n","prompt = PromptTemplate(\n","    input_variables=[\"ciudad\", \"temperatura\", \"humedad\", \"presion\"],\n","    template=template\n",")\n","\n","# Generamos un prompt espec칤fico con valores concretos\n","prompt_formateado = prompt.format(\n","    ciudad=\"Medell칤n\",\n","    temperatura=22,\n","    humedad=75,\n","    presion=1013\n",")\n","\n","print(\"Prompt generado:\")\n","print(prompt_formateado)"]},{"cell_type":"markdown","id":"1248de18","metadata":{"id":"1248de18"},"source":["### 3.2 ChatPromptTemplate (Plantillas para Conversaciones)\n","\n","ChatPromptTemplate es la evoluci칩n para modelos de chat. Permite definir m칰ltiples mensajes con diferentes roles y variables en cada uno."]},{"cell_type":"code","execution_count":null,"id":"2329227e","metadata":{"id":"2329227e"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# Definir una plantilla de chat con m칰ltiples mensajes\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un {profesion} especializado en {especialidad}.\"),\n","    (\"human\", \"쮺u치l es tu an치lisis sobre {tema}?\"),\n","    (\"ai\", \"Bas치ndome en mi experiencia, puedo decirte que...\"),\n","    (\"human\", \"{pregunta_especifica}\")\n","])\n","\n","# Generar mensajes con valores espec칤ficos\n","mensajes = chat_template.format_messages(\n","    profesion=\"cient칤fico atmosf칠rico\",\n","    especialidad=\"fen칩menos de El Ni침o y La Ni침a\",\n","    tema=\"el impacto del cambio clim치tico en los patrones de lluvia\",\n","    pregunta_especifica=\"쯈u칠 podemos esperar en los pr칩ximos 10 a침os para Latinoam칠rica?\"\n",")\n","\n","# Veamos los mensajes generados\n","for i, mensaje in enumerate(mensajes):\n","    print(f\"\\nMensaje {i+1} ({mensaje.type}):\")\n","    print(mensaje.content)\n","    print(\"-\" * 50)"]},{"cell_type":"code","execution_count":null,"id":"f827b04a","metadata":{"id":"f827b04a"},"outputs":[],"source":["# Ahora usemos esta plantilla con nuestro modelo\n","respuesta_final = llm.invoke(mensajes)\n","print(\"\\nRespuesta del modelo:\")\n","print(respuesta_final.content)"]},{"cell_type":"markdown","id":"05c5b9de","metadata":{"id":"05c5b9de"},"source":["### 3.3 FewShotChatMessagePromptTemplate (Aprendizaje por Ejemplos)\n","\n","El Few-Shot Learning es una t칠cnica donde proporcionamos ejemplos al modelo para que entienda mejor el formato o estilo de respuesta que esperamos. Es especialmente 칰til para tareas estructuradas o con formato espec칤fico."]},{"cell_type":"code","execution_count":null,"id":"b79a021b","metadata":{"id":"b79a021b"},"outputs":[],"source":["from langchain_core.prompts import FewShotChatMessagePromptTemplate\n","\n","# Definimos ejemplos de c칩mo queremos que el modelo responda\n","# Cada ejemplo es un diccionario con las variables y sus valores\n","ejemplos = [\n","    {\n","        \"condicion\": \"cielo despejado, 28춿C, baja humedad\",\n","        \"recomendacion\": \"APTO para actividades al aire libre. Hidrataci칩n importante. Protector solar recomendado. Nivel de riesgo: BAJO\"\n","    },\n","    {\n","        \"condicion\": \"lluvia intensa, 18춿C, alta humedad\",\n","        \"recomendacion\": \"NO APTO para actividades al aire libre. Permanecer en interiores. Riesgo de inundaciones. Nivel de riesgo: ALTO\"\n","    },\n","    {\n","        \"condicion\": \"nublado, 22춿C, humedad moderada\",\n","        \"recomendacion\": \"APTO con precauciones. Llevar paraguas por si acaso. Temperatura confortable. Nivel de riesgo: MEDIO\"\n","    }\n","]\n","\n","# Definir el formato de cada ejemplo\n","ejemplo_prompt = ChatPromptTemplate.from_messages([\n","    (\"human\", \"Condiciones: {condicion}\"),\n","    (\"ai\", \"{recomendacion}\")\n","])\n","\n","# Crear el Few-Shot Prompt\n","few_shot_prompt = FewShotChatMessagePromptTemplate(\n","    example_prompt=ejemplo_prompt,\n","    examples=ejemplos,\n",")\n","\n","# Crear el prompt final que incluye los ejemplos y la nueva pregunta\n","prompt_final = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un sistema de recomendaciones meteorol칩gicas. Analiza las condiciones y proporciona recomendaciones claras siguiendo el formato de los ejemplos.\"),\n","    few_shot_prompt,\n","    (\"human\", \"Condiciones: {condicion_nueva}\")\n","])\n","\n","# Probar con una nueva condici칩n\n","respuesta = llm.invoke(\n","    prompt_final.format_messages(\n","        condicion_nueva=\"viento fuerte de 45 km/h, 15춿C, cielo parcialmente nublado\"\n","    )\n",")\n","\n","print(\"Recomendaci칩n basada en ejemplos:\")\n","print(respuesta.content)"]},{"cell_type":"markdown","id":"3320efda","metadata":{"id":"3320efda"},"source":["### Ejercicio Pr치ctico 2\n","\n","Modifica el c칩digo anterior para crear un sistema de clasificaci칩n de fen칩menos meteorol칩gicos. Agrega tres ejemplos de fen칩menos (tornado, tormenta el칠ctrica, nevada) con sus caracter칤sticas y nivel de peligrosidad. Luego prueba clasificar: \"vientos rotacionales de 120 km/h, nubes oscuras en forma de embudo\"."]},{"cell_type":"markdown","id":"e3ff1ac0","metadata":{"id":"e3ff1ac0"},"source":["---\n","\n","## 4. Chains (Cadenas): Encadenamiento de Operaciones\n","\n","Las cadenas son el coraz칩n de LangChain. Permiten conectar m칰ltiples componentes (prompts, modelos, parsers) en un flujo de procesamiento.\n","\n","### Concepto: LCEL (LangChain Expression Language)\n","\n","En LangChain 1.0+, usamos el operador `|` (pipe) para encadenar componentes. Esto crea flujos de datos claros y componibles."]},{"cell_type":"code","execution_count":null,"id":"5ff70d87","metadata":{"id":"5ff70d87"},"outputs":[],"source":["# Crear una cadena simple: Prompt -> Modelo\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Definimos nuestro prompt\n","prompt_cadena = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un analista de datos meteorol칩gicos. Proporciona respuestas concisas y precisas.\"),\n","    (\"human\", \"{consulta}\")\n","])\n","\n","# Creamos la cadena usando el operador pipe (|)\n","# El flujo es: entrada -> prompt -> modelo -> parser de string\n","cadena = prompt_cadena | llm | StrOutputParser()\n","\n","# Ejecutar la cadena es tan simple como invocarla\n","resultado = cadena.invoke({\"consulta\": \"쯈u칠 es la presi칩n atmosf칠rica y c칩mo se mide?\"})\n","\n","print(resultado)"]},{"cell_type":"markdown","id":"6c490bf4","metadata":{"id":"6c490bf4"},"source":["### Cadenas Secuenciales\n","\n","Podemos crear cadenas m치s complejas donde la salida de una se convierte en la entrada de la siguiente."]},{"cell_type":"code","execution_count":null,"id":"8a6de136","metadata":{"id":"8a6de136"},"outputs":[],"source":["from langchain_core.runnables import RunnablePassthrough\n","\n","# Primera cadena: genera un pron칩stico meteorol칩gico\n","prompt_pronostico = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un meteor칩logo. Genera un pron칩stico t칠cnico breve para la ciudad solicitada.\"),\n","    (\"human\", \"Ciudad: {ciudad}\")\n","])\n","\n","cadena_pronostico = prompt_pronostico | llm | StrOutputParser()\n","\n","# Segunda cadena: traduce el pron칩stico a lenguaje simple\n","prompt_simplificacion = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un comunicador experto. Toma informaci칩n t칠cnica y expl칤cala de forma que cualquier persona pueda entenderla.\"),\n","    (\"human\", \"Informaci칩n t칠cnica: {pronostico_tecnico}\")\n","])\n","\n","cadena_simplificacion = prompt_simplificacion | llm | StrOutputParser()\n","\n","# Combinar ambas cadenas\n","# RunnablePassthrough permite pasar datos entre cadenas\n","cadena_completa = (\n","    {\"pronostico_tecnico\": cadena_pronostico}\n","    | cadena_simplificacion\n",")\n","\n","# Ejecutar la cadena completa\n","resultado_final = cadena_completa.invoke({\"ciudad\": \"Bogot치\"})\n","\n","print(\"Pron칩stico simplificado:\")\n","print(resultado_final)"]},{"cell_type":"markdown","id":"adb99d1d","metadata":{"id":"adb99d1d"},"source":["---\n","\n","## 5. Output Parsers: Estructurando las Respuestas\n","\n","Los parsers transforman la salida de texto del modelo en estructuras de datos 칰tiles. Son fundamentales para integrar LLMs en aplicaciones reales.\n","\n","### 5.1 StrOutputParser (Parser de String)\n","\n","Ya lo hemos usado. Simplemente extrae el contenido de texto de la respuesta del modelo."]},{"cell_type":"markdown","id":"7a52a7dc","metadata":{"id":"7a52a7dc"},"source":["### 5.2 JsonOutputParser (Parser JSON)\n","\n","Convierte la respuesta del modelo en un objeto JSON. 칔til cuando necesitamos datos estructurados."]},{"cell_type":"code","execution_count":null,"id":"ee265d82","metadata":{"id":"ee265d82"},"outputs":[],"source":["from langchain_core.output_parsers import JsonOutputParser\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","\n","# Definimos la estructura que esperamos\n","class PronosticoMeteorologico(BaseModel):\n","    temperatura_max: int = Field(description=\"Temperatura m치xima en grados Celsius\")\n","    temperatura_min: int = Field(description=\"Temperatura m칤nima en grados Celsius\")\n","    probabilidad_lluvia: int = Field(description=\"Probabilidad de lluvia en porcentaje\")\n","    condicion: str = Field(description=\"Descripci칩n de la condici칩n clim치tica\")\n","    recomendacion: str = Field(description=\"Recomendaci칩n para el d칤a\")\n","\n","# Crear el parser\n","parser = JsonOutputParser(pydantic_object=PronosticoMeteorologico)\n","\n","# Crear prompt con instrucciones de formato\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un sistema meteorol칩gico. Responde SIEMPRE en el formato JSON especificado.\\n{format_instructions}\"),\n","    (\"human\", \"Genera un pron칩stico para {ciudad} para ma침ana\")\n","])\n","\n","# Crear la cadena\n","cadena = prompt | llm | parser\n","\n","# Ejecutar\n","resultado = cadena.invoke({\n","    \"ciudad\": \"Cartagena\",\n","    \"format_instructions\": parser.get_format_instructions()\n","})\n","\n","print(\"Tipo de dato:\", type(resultado))\n","print(\"\\nDatos estructurados:\")\n","for clave, valor in resultado.items():\n","    print(f\"  {clave}: {valor}\")"]},{"cell_type":"markdown","id":"47b2f81c","metadata":{"id":"47b2f81c"},"source":["### 5.3 PydanticOutputParser (Parser con Validaci칩n)\n","\n","Similar al JsonOutputParser pero con validaci칩n de tipos m치s estricta."]},{"cell_type":"code","execution_count":null,"id":"63e0e9e9","metadata":{"id":"63e0e9e9"},"outputs":[],"source":["from pydantic import BaseModel, Field\n","from langchain_core.output_parsers import PydanticOutputParser\n","from typing import List\n","\n","class AlertaMeteorologica(BaseModel):\n","    tipo_alerta: str = Field(description=\"Tipo de alerta: amarilla, naranja o roja\")\n","    fenomeno: str = Field(description=\"Fen칩meno meteorol칩gico\")\n","    zonas_afectadas: List[str] = Field(description=\"Lista de zonas afectadas\")\n","    duracion_horas: int = Field(description=\"Duraci칩n estimada en horas\")\n","    medidas_precaucion: List[str] = Field(description=\"Medidas de precauci칩n recomendadas\")\n","\n","# Crear parser\n","parser_pydantic = PydanticOutputParser(pydantic_object=AlertaMeteorologica)\n","\n","# Crear prompt\n","prompt_alerta = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un sistema de alertas meteorol칩gicas. Genera alertas en el formato especificado.\\n{format_instructions}\"),\n","    (\"human\", \"Genera una alerta para: {situacion}\")\n","])\n","\n","# Cadena\n","cadena_alerta = prompt_alerta | llm | parser_pydantic\n","\n","# Ejecutar\n","alerta = cadena_alerta.invoke({\n","    \"situacion\": \"Tormenta el칠ctrica severa aproxim치ndose a la regi칩n andina\",\n","    \"format_instructions\": parser_pydantic.get_format_instructions()\n","})\n","\n","print(\"Objeto Pydantic generado:\")\n","print(f\"Tipo de alerta: {alerta.tipo_alerta}\")\n","print(f\"Fen칩meno: {alerta.fenomeno}\")\n","print(f\"Zonas: {', '.join(alerta.zonas_afectadas)}\")\n","print(f\"Duraci칩n: {alerta.duracion_horas} horas\")\n","print(f\"Medidas de precauci칩n:\")\n","for medida in alerta.medidas_precaucion:\n","    print(f\"  - {medida}\")"]},{"cell_type":"markdown","id":"17f01bae","metadata":{"id":"17f01bae"},"source":["### 5.4 CommaSeparatedListOutputParser\n","\n","칔til cuando necesitamos listas simples de elementos."]},{"cell_type":"code","execution_count":null,"id":"6320e1c9","metadata":{"id":"6320e1c9"},"outputs":[],"source":["from langchain_core.output_parsers import CommaSeparatedListOutputParser\n","\n","# Crear parser\n","parser_lista = CommaSeparatedListOutputParser()\n","\n","# Prompt simple\n","prompt_lista = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un experto meteorol칩gico.\"),\n","    (\"human\", \"Lista 5 instrumentos usados para medir variables meteorol칩gicas.\\n{format_instructions}\")\n","])\n","\n","# Cadena\n","cadena_lista = prompt_lista | llm | parser_lista\n","\n","# Ejecutar\n","instrumentos = cadena_lista.invoke({\n","    \"format_instructions\": parser_lista.get_format_instructions()\n","})\n","\n","print(\"Tipo:\", type(instrumentos))\n","print(\"\\nInstrumentos meteorol칩gicos:\")\n","for i, instrumento in enumerate(instrumentos, 1):\n","    print(f\"{i}. {instrumento}\")"]},{"cell_type":"markdown","id":"aa955a97","metadata":{"id":"aa955a97"},"source":["### Ejercicio Pr치ctico 3\n","\n","Crea una cadena que reciba el nombre de una ciudad y retorne un objeto JSON con: nombre_ciudad, latitud (aproximada), longitud (aproximada), zona_climatica, y temperatura_promedio_anual. Usa JsonOutputParser o PydanticOutputParser."]},{"cell_type":"markdown","id":"aa74b226","metadata":{"id":"aa74b226"},"source":["---\n","\n","## 6. Memoria Conversacional\n","\n","La memoria permite que las aplicaciones mantengan contexto entre m칰ltiples interacciones. Es esencial para crear chatbots y asistentes conversacionales.\n","\n","### 6.1 ConversationBufferMemory (Memoria de Buffer Simple)\n","\n","Almacena todos los mensajes de la conversaci칩n en memoria."]},{"cell_type":"code","execution_count":null,"id":"a7b681bb","metadata":{"id":"a7b681bb"},"outputs":[],"source":["from langchain.memory import ConversationBufferMemory\n","from langchain_core.messages import HumanMessage, AIMessage\n","\n","# Crear memoria\n","memoria = ConversationBufferMemory(return_messages=True)\n","\n","# Agregar mensajes a la memoria\n","memoria.chat_memory.add_user_message(\"쮺u치l es la temperatura promedio en el Amazonas?\")\n","memoria.chat_memory.add_ai_message(\"La temperatura promedio en la regi칩n amaz칩nica es de aproximadamente 26-28춿C durante todo el a침o, con alta humedad.\")\n","\n","memoria.chat_memory.add_user_message(\"쯏 cu치nto llueve all칤?\")\n","memoria.chat_memory.add_ai_message(\"La regi칩n amaz칩nica recibe entre 2000 y 3000 mm de lluvia al a침o, siendo una de las zonas m치s lluviosas del planeta.\")\n","\n","# Ver el historial\n","print(\"Historial de conversaci칩n:\")\n","for mensaje in memoria.chat_memory.messages:\n","    print(f\"{mensaje.type}: {mensaje.content}\\n\")"]},{"cell_type":"markdown","id":"7e42df44","metadata":{"id":"7e42df44"},"source":["### 6.2 ConversationBufferWindowMemory (Memoria con Ventana)\n","\n","Mantiene solo los 칰ltimos N mensajes. 칔til para limitar el consumo de tokens."]},{"cell_type":"code","execution_count":null,"id":"cddef44c","metadata":{"id":"cddef44c"},"outputs":[],"source":["from langchain.memory import ConversationBufferWindowMemory\n","\n","# Crear memoria con ventana de 2 interacciones (4 mensajes)\n","memoria_ventana = ConversationBufferWindowMemory(k=2, return_messages=True)\n","\n","# Agregar varios mensajes\n","conversaciones = [\n","    (\"쯈u칠 es un hurac치n?\", \"Un hurac치n es un sistema de baja presi칩n con vientos superiores a 119 km/h.\"),\n","    (\"쮻칩nde se forman?\", \"Se forman en aguas tropicales c치lidas, principalmente entre 5춿 y 20춿 de latitud.\"),\n","    (\"쮺u치nto duran?\", \"Pueden durar desde d칤as hasta semanas, dependiendo de las condiciones.\"),\n","    (\"쯉on peligrosos?\", \"S칤, son extremadamente peligrosos por sus vientos, lluvias intensas e inundaciones.\")\n","]\n","\n","for pregunta, respuesta in conversaciones:\n","    memoria_ventana.chat_memory.add_user_message(pregunta)\n","    memoria_ventana.chat_memory.add_ai_message(respuesta)\n","\n","# Verificar que solo mantiene los 칰ltimos 2 intercambios (4 mensajes)\n","print(f\"Mensajes en memoria: {len(memoria_ventana.chat_memory.messages)}\")\n","print(\"\\n칔ltimos mensajes recordados:\")\n","for mensaje in memoria_ventana.chat_memory.messages:\n","    print(f\"{mensaje.type}: {mensaje.content}\")"]},{"cell_type":"markdown","id":"b0f4e5b0","metadata":{"id":"b0f4e5b0"},"source":["### 6.3 ConversationSummaryMemory (Memoria con Resumen)\n","\n","En lugar de almacenar mensajes completos, crea res칰menes. Muy 칰til para conversaciones largas."]},{"cell_type":"code","execution_count":null,"id":"7ee64ac9","metadata":{"id":"7ee64ac9"},"outputs":[],"source":["from langchain.memory import ConversationSummaryMemory\n","\n","# Crear memoria con resumen (requiere un LLM para generar res칰menes)\n","memoria_resumen = ConversationSummaryMemory(llm=llm, return_messages=True)\n","\n","# Simular una conversaci칩n larga sobre meteorolog칤a\n","conversacion_larga = [\n","    (\"Expl칤came qu칠 es la atm칩sfera\",\n","     \"La atm칩sfera es la capa de gases que rodea la Tierra, compuesta principalmente por nitr칩geno (78%) y ox칤geno (21%). Se divide en cinco capas: troposfera, estratosfera, mesosfera, termosfera y exosfera.\"),\n","    (\"쯈u칠 sucede en la troposfera?\",\n","     \"La troposfera es la capa m치s baja, donde ocurren todos los fen칩menos meteorol칩gicos. Se extiende hasta aproximadamente 10-15 km de altura y contiene el 80% de la masa atmosf칠rica.\"),\n","    (\"쯇or qu칠 es importante la estratosfera?\",\n","     \"La estratosfera contiene la capa de ozono, que absorbe la radiaci칩n ultravioleta da침ina del sol, protegiendo la vida en la Tierra. Se extiende de 10 a 50 km de altura.\"),\n","]\n","\n","for pregunta, respuesta in conversacion_larga:\n","    memoria_resumen.save_context(\n","        {\"input\": pregunta},\n","        {\"output\": respuesta}\n","    )\n","\n","# Ver el resumen generado\n","print(\"Resumen de la conversaci칩n:\")\n","print(memoria_resumen.load_memory_variables({}))"]},{"cell_type":"markdown","id":"e14354ee","metadata":{"id":"e14354ee"},"source":["### 6.4 Uso de Memoria en Agentes\n","\n","Para agentes, la memoria se integra de forma especial. Usaremos memoria de mensajes directamente en el historial del agente."]},{"cell_type":"code","execution_count":null,"id":"e0489162","metadata":{"id":"e0489162"},"outputs":[],"source":["# En agentes modernos, la memoria se maneja con un chat_history\n","# Veremos esto en detalle en la secci칩n de agentes\n","# Por ahora, entendamos el concepto:\n","\n","from langchain_core.chat_history import InMemoryChatMessageHistory\n","\n","# Crear un historial de chat en memoria\n","historial_chat = InMemoryChatMessageHistory()\n","\n","# Agregar mensajes\n","historial_chat.add_user_message(\"쯈u칠 temperatura hace hoy?\")\n","historial_chat.add_ai_message(\"Hoy la temperatura es de 24춿C con cielo parcialmente nublado.\")\n","historial_chat.add_user_message(\"쯃lover치?\")\n","historial_chat.add_ai_message(\"Hay un 30% de probabilidad de lluvia en la tarde.\")\n","\n","# Ver mensajes\n","print(\"Historial completo:\")\n","for mensaje in historial_chat.messages:\n","    print(f\"  [{mensaje.type}]: {mensaje.content}\")"]},{"cell_type":"markdown","id":"b12885a4","metadata":{"id":"b12885a4"},"source":["---\n","\n","## 7. Serializaci칩n de Prompts\n","\n","La serializaci칩n permite guardar y cargar prompts, facilitando la reutilizaci칩n y versionado.\n","\n"]},{"cell_type":"markdown","id":"4414c8b8","metadata":{"id":"4414c8b8"},"source":["### Guardar y cargar Prompts desde JSON"]},{"cell_type":"code","execution_count":null,"id":"de3b0c3e","metadata":{"id":"de3b0c3e"},"outputs":[],"source":["# Si prefieres JSON, usa este enfoque:\n","import json\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# 1. GUARDAR manualmente\n","prompt_meteorologia = ChatPromptTemplate.from_messages([\n","    (\"system\", \"\"\"Eres un asistente meteorol칩gico especializado.\n","    Tu trabajo es proporcionar informaci칩n precisa y 칰til sobre el clima.\n","    Siempre incluye unidades de medida y contexto relevante.\"\"\"),\n","    (\"human\", \"{pregunta_usuario}\")\n","])\n","\n","# Convertir a formato serializable\n","prompt_dict = {\n","    \"_type\": \"prompt\",\n","    \"input_variables\": [\"pregunta_usuario\"],\n","    \"messages\": [\n","        {\n","            \"type\": \"system\",\n","            \"content\": \"\"\"Eres un asistente meteorol칩gico especializado.\n","    Tu trabajo es proporcionar informaci칩n precisa y 칰til sobre el clima.\n","    Siempre incluye unidades de medida y contexto relevante.\"\"\"\n","        },\n","        {\n","            \"type\": \"human\",\n","            \"content\": \"{pregunta_usuario}\"\n","        }\n","    ]\n","}\n","\n","with open('prompt_meteorologia.json', 'w', encoding='utf-8') as f:\n","    json.dump(prompt_dict, f, indent=2, ensure_ascii=False)\n","\n","print(\"Prompt guardado en JSON\")\n","\n","# 2. CARGAR desde JSON\n","with open('prompt_meteorologia.json', 'r', encoding='utf-8') as f:\n","    data = json.load(f)\n","\n","# Reconstruir usando from_messages\n","mensajes = [(msg[\"type\"], msg[\"content\"]) for msg in data[\"messages\"]]\n","prompt_reconstruido = ChatPromptTemplate.from_messages(mensajes)\n","\n","# 3. Usar\n","respuesta = llm.invoke(\n","    prompt_reconstruido.format_messages(\n","        pregunta_usuario=\"쯈u칠 es la humedad relativa y c칩mo se relaciona con la sensaci칩n t칠rmica?\"\n","    )\n",")\n","\n","print(\"\\nRespuesta:\")\n","print(respuesta.content)"]},{"cell_type":"markdown","id":"948793d0","metadata":{"id":"948793d0"},"source":["### Reflexi칩n\n","\n","La serializaci칩n es crucial para:\n","- Mantener versiones de prompts en control de versiones\n","- Compartir prompts entre equipos\n","- Facilitar el testing A/B de diferentes versiones\n","- Separar la l칩gica del prompt del c칩digo de la aplicaci칩n"]},{"cell_type":"markdown","id":"f87c9416","metadata":{"id":"f87c9416"},"source":["---\n","\n","## 8. Herramientas (Tools)\n","\n","Las herramientas permiten a los agentes interactuar con el mundo exterior: realizar c치lculos, buscar informaci칩n, ejecutar c칩digo, etc.\n","\n","### 8.1 Concepto de Herramientas\n","\n","Una herramienta es una funci칩n que el agente puede invocar. LangChain proporciona:\n","- Herramientas predefinidas (b칰squeda web, calculadoras, APIs, etc.)\n","- Decoradores para crear herramientas personalizadas\n","- Integraci칩n con m칰ltiples servicios\n","\n","### 8.2 Crear Herramientas Personalizadas con @tool"]},{"cell_type":"code","execution_count":null,"id":"c595c87a","metadata":{"id":"c595c87a"},"outputs":[],"source":["from langchain_core.tools import tool\n","\n","# Herramienta 1: Convertir temperatura\n","@tool\n","def convertir_temperatura(temperatura: float, de_unidad: str, a_unidad: str) -> str:\n","    \"\"\"Convierte temperatura entre Celsius, Fahrenheit y Kelvin.\n","\n","    Args:\n","        temperatura: El valor num칠rico de la temperatura\n","        de_unidad: La unidad origen (celsius, fahrenheit, kelvin)\n","        a_unidad: La unidad destino (celsius, fahrenheit, kelvin)\n","\n","    Returns:\n","        String con el resultado de la conversi칩n\n","    \"\"\"\n","    de_unidad = de_unidad.lower()\n","    a_unidad = a_unidad.lower()\n","\n","    # Primero convertir todo a Celsius\n","    if de_unidad == \"celsius\":\n","        temp_celsius = temperatura\n","    elif de_unidad == \"fahrenheit\":\n","        temp_celsius = (temperatura - 32) * 5/9\n","    elif de_unidad == \"kelvin\":\n","        temp_celsius = temperatura - 273.15\n","    else:\n","        return \"Unidad no reconocida\"\n","\n","    # Luego convertir de Celsius a la unidad destino\n","    if a_unidad == \"celsius\":\n","        resultado = temp_celsius\n","    elif a_unidad == \"fahrenheit\":\n","        resultado = (temp_celsius * 9/5) + 32\n","    elif a_unidad == \"kelvin\":\n","        resultado = temp_celsius + 273.15\n","    else:\n","        return \"Unidad no reconocida\"\n","\n","    return f\"{temperatura}춿{de_unidad[0].upper()} = {resultado:.2f}춿{a_unidad[0].upper()}\"\n","\n","# Herramienta 2: Calcular punto de roc칤o\n","@tool\n","def calcular_punto_rocio(temperatura: float, humedad_relativa: float) -> str:\n","    \"\"\"Calcula el punto de roc칤o aproximado.\n","\n","    Args:\n","        temperatura: Temperatura en grados Celsius\n","        humedad_relativa: Humedad relativa en porcentaje (0-100)\n","\n","    Returns:\n","        String con el punto de roc칤o calculado\n","    \"\"\"\n","    # F칩rmula de Magnus simplificada\n","    a = 17.27\n","    b = 237.7\n","\n","    alpha = ((a * temperatura) / (b + temperatura)) + (humedad_relativa / 100.0)\n","    punto_rocio = (b * alpha) / (a - alpha)\n","\n","    return f\"Con temperatura de {temperatura}춿C y humedad de {humedad_relativa}%, el punto de roc칤o es aproximadamente {punto_rocio:.2f}춿C\"\n","\n","# Herramienta 3: Clasificar 칤ndice UV\n","@tool\n","def clasificar_indice_uv(indice_uv: float) -> str:\n","    \"\"\"Clasifica el 칤ndice UV y proporciona recomendaciones.\n","\n","    Args:\n","        indice_uv: Valor del 칤ndice UV (0-11+)\n","\n","    Returns:\n","        Clasificaci칩n y recomendaciones\n","    \"\"\"\n","    if indice_uv <= 2:\n","        return f\"칈ndice UV {indice_uv}: BAJO. No se requiere protecci칩n especial.\"\n","    elif indice_uv <= 5:\n","        return f\"칈ndice UV {indice_uv}: MODERADO. Use protector solar. Busque sombra durante el mediod칤a.\"\n","    elif indice_uv <= 7:\n","        return f\"칈ndice UV {indice_uv}: ALTO. Protecci칩n esencial. Use sombrero y gafas de sol.\"\n","    elif indice_uv <= 10:\n","        return f\"칈ndice UV {indice_uv}: MUY ALTO. Protecci칩n extra. Evite exposici칩n al sol entre 10am-4pm.\"\n","    else:\n","        return f\"칈ndice UV {indice_uv}: EXTREMO. Evite la exposici칩n solar. Permanezca en interiores si es posible.\"\n","\n","# Probar las herramientas\n","print(\"Prueba de herramientas:\\n\")\n","print(convertir_temperatura.invoke({\"temperatura\": 25, \"de_unidad\": \"celsius\", \"a_unidad\": \"fahrenheit\"}))\n","print(calcular_punto_rocio.invoke({\"temperatura\": 25, \"humedad_relativa\": 60}))\n","print(clasificar_indice_uv.invoke({\"indice_uv\": 8.5}))"]},{"cell_type":"markdown","id":"0758bd5a","metadata":{"id":"0758bd5a"},"source":["### 8.3 Anatom칤a de una Herramienta\n","\n","Observa que cada herramienta tiene:\n","\n","1. **Decorador @tool**: Convierte la funci칩n en una herramienta de LangChain\n","2. **Docstring detallado**: El modelo lo usa para entender cu치ndo y c칩mo usar la herramienta\n","3. **Type hints**: Especifica los tipos de los par치metros\n","4. **L칩gica de negocio**: El c칩digo que ejecuta la herramienta\n","5. **Return claro**: Siempre retorna un string explicativo\n","\n","### 8.4 Ver Informaci칩n de las Herramientas"]},{"cell_type":"code","execution_count":null,"id":"e84adc4e","metadata":{"id":"e84adc4e"},"outputs":[],"source":["# Ver los metadatos de una herramienta\n","print(\"Nombre de la herramienta:\", convertir_temperatura.name)\n","print(\"\\nDescripci칩n:\")\n","print(convertir_temperatura.description)\n","print(\"\\nArgumentos esperados:\")\n","print(convertir_temperatura.args)"]},{"cell_type":"markdown","id":"015d6623","metadata":{"id":"015d6623"},"source":["### Ejercicio Pr치ctico 4\n","\n","Crea una herramienta llamada `calcular_sensacion_termica` que reciba temperatura y velocidad del viento, y calcule la sensaci칩n t칠rmica usando la f칩rmula simplificada:\n","\n","Sensaci칩n = Temperatura - (Velocidad_viento 칑 0.7)\n","\n","Incluye un docstring completo y prueba tu herramienta."]},{"cell_type":"markdown","id":"76a356f8","metadata":{"id":"76a356f8"},"source":["---\n","\n","## 9. Agentes: Razonamiento y Acci칩n\n","\n","Los agentes son sistemas que pueden razonar sobre qu칠 hacer y tomar decisiones aut칩nomas sobre qu칠 herramientas usar.\n","\n","### 9.1 쯈u칠 es un Agente?\n","\n","Un agente es un sistema que:\n","1. Recibe una tarea o pregunta\n","2. Razona sobre c칩mo resolverla\n","3. Decide qu칠 herramientas usar\n","4. Ejecuta las herramientas necesarias\n","5. Analiza los resultados\n","6. Repite el proceso hasta resolver la tarea\n"]},{"cell_type":"markdown","id":"562551f8","metadata":{"id":"562551f8"},"source":["### 9.2 Crear un Agente con LangChain\n","\n","Vamos a crear un agente meteorol칩gico que use nuestras herramientas."]},{"cell_type":"code","execution_count":null,"id":"4c2cc86c","metadata":{"id":"4c2cc86c"},"outputs":[],"source":["from langchain.agents import AgentExecutor, create_tool_calling_agent\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# Definir las herramientas que el agente puede usar\n","herramientas = [\n","    convertir_temperatura,\n","    calcular_punto_rocio,\n","    clasificar_indice_uv\n","]\n","\n","# Crear el prompt para el agente\n","prompt_agente = ChatPromptTemplate.from_messages([\n","    (\"system\", \"\"\"Eres un asistente meteorol칩gico experto.\n","    Tienes acceso a herramientas especializadas para realizar c치lculos y an치lisis.\n","\n","    Cuando necesites realizar conversiones de temperatura, calcular punto de roc칤o,\n","    o evaluar 칤ndices UV, usa las herramientas disponibles.\n","\n","    Siempre proporciona respuestas claras y profesionales.\"\"\"),\n","    (\"placeholder\", \"{chat_history}\"),\n","    (\"human\", \"{input}\"),\n","    (\"placeholder\", \"{agent_scratchpad}\"),\n","])\n","\n","# Crear el agente\n","agente = create_tool_calling_agent(\n","    llm=llm,\n","    tools=herramientas,\n","    prompt=prompt_agente\n",")\n","\n","# Crear el executor del agente\n","agente_meteorologico = AgentExecutor(\n","    agent=agente,\n","    tools=herramientas,\n","    verbose=True,\n","    handle_parsing_errors=True,\n","    max_iterations=5,\n",")\n","\n","print(\"Agente meteorol칩gico creado exitosamente\")"]},{"cell_type":"markdown","id":"4ef0df51","metadata":{"id":"4ef0df51"},"source":["### 9.3 Ejecutar el Agente\n","\n","Ahora pondremos al agente a trabajar con diferentes consultas."]},{"cell_type":"code","execution_count":null,"id":"2ac3ad5c","metadata":{"id":"2ac3ad5c"},"outputs":[],"source":["# Consulta 1: Uso de herramienta 칰nica\n","consulta1 = \"Necesito saber cu치ntos grados Fahrenheit son 30 grados Celsius\"\n","\n","print(\"=\"*60)\n","print(\"CONSULTA 1:\", consulta1)\n","print(\"=\"*60)\n","\n","# CORRECCI칍N: AgentExecutor usa {\"input\": ...} no {\"messages\": ...}\n","resultado1 = agente_meteorologico.invoke({\"input\": consulta1})\n","\n","# El resultado es un diccionario con la salida final\n","print(\"\\n\" + \"=\"*60)\n","print(\"RESPUESTA FINAL:\")\n","print(\"=\"*60)\n","print(resultado1[\"output\"])"]},{"cell_type":"code","execution_count":null,"id":"2876b609","metadata":{"id":"2876b609"},"outputs":[],"source":["# Consulta 2: Uso de m칰ltiples herramientas\n","consulta2 = \"\"\"Hoy la temperatura es de 28춿C con 70% de humedad relativa,\n","y el 칤ndice UV es 9. Dame un reporte completo.\"\"\"\n","\n","print(\"\\n\\n\" + \"=\"*60)\n","print(\"CONSULTA 2:\", consulta2)\n","print(\"=\"*60)\n","\n","resultado2 = agente_meteorologico.invoke({\"input\": consulta2})\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"RESPUESTA FINAL:\")\n","print(\"=\"*60)\n","print(resultado2[\"output\"])"]},{"cell_type":"markdown","id":"b636eb3a","metadata":{"id":"b636eb3a"},"source":["### 9.4 Agente con Memoria Conversacional\n","\n","Agreguemos memoria para que el agente recuerde interacciones previas."]},{"cell_type":"code","execution_count":null,"id":"91134687","metadata":{"id":"91134687"},"outputs":[],"source":["from langchain.memory import ConversationBufferMemory\n","\n","# Crear memoria conversacional\n","memoria = ConversationBufferMemory(\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")\n","\n","# Crear agente con memoria\n","agente_con_memoria = AgentExecutor(\n","    agent=agente,\n","    tools=herramientas,\n","    memory=memoria,\n","    verbose=True,\n","    handle_parsing_errors=True,\n",")\n","\n","# Primera interacci칩n\n","print(\"CONVERSACI칍N CON MEMORIA\")\n","print(\"=\"*60)\n","\n","pregunta1 = \"La temperatura actual es 25춿C. Convi칠rtela a Fahrenheit.\"\n","print(f\"\\nUSUARIO: {pregunta1}\")\n","\n","resultado = agente_con_memoria.invoke({\"input\": pregunta1})\n","print(f\"AGENTE: {resultado['output']}\")\n","\n","# Segunda interacci칩n (el agente recuerda la temperatura anterior)\n","pregunta2 = \"쮺u치l ser칤a el punto de roc칤o con esa temperatura si la humedad es 65%?\"\n","print(f\"\\nUSUARIO: {pregunta2}\")\n","\n","resultado2 = agente_con_memoria.invoke({\"input\": pregunta2})\n","print(f\"AGENTE: {resultado2['output']}\")"]},{"cell_type":"markdown","id":"45b0cef1","metadata":{"id":"45b0cef1"},"source":["### Reflexi칩n sobre Agentes\n","\n","Observa c칩mo el agente:\n","1. **Entiende el contexto**: Interpreta lo que el usuario necesita\n","2. **Planifica**: Decide qu칠 herramientas usar y en qu칠 orden\n","3. **Ejecuta**: Llama a las herramientas con los par치metros correctos\n","4. **Sintetiza**: Combina los resultados en una respuesta coherente\n","5. **Recuerda**: Mantiene contexto entre interacciones (con memoria)\n","\n"]},{"cell_type":"markdown","id":"f5948778","metadata":{"id":"f5948778"},"source":["### Ejercicio Pr치ctico 5\n","\n","Crea una nueva herramienta llamada `evaluar_riesgo_lluvia` que reciba temperatura y humedad, y retorne un nivel de riesgo (bajo, medio, alto). Agr칠gala al agente y prueba con: \"La temperatura es 22춿C y la humedad 85%. 쮿ay riesgo de lluvia?\""]},{"cell_type":"markdown","id":"fb10f9ef","metadata":{"id":"fb10f9ef"},"source":["---\n","\n","## 10. Proyecto Final: Sistema Integrado de An치lisis Meteorol칩gico\n","\n","Ahora que conoces todos los componentes, vamos a construir un sistema completo que integre todo lo aprendido."]},{"cell_type":"code","execution_count":null,"id":"7e7ed552","metadata":{"id":"7e7ed552"},"outputs":[],"source":["# Sistema completo de an치lisis meteorol칩gico\n","\n","# 1. Definir herramientas adicionales\n","@tool\n","def analizar_condiciones_completas(temperatura: float, humedad: float, presion: float, viento: float) -> str:\n","    \"\"\"Analiza un conjunto completo de condiciones meteorol칩gicas.\n","\n","    Args:\n","        temperatura: Temperatura en grados Celsius\n","        humedad: Humedad relativa en porcentaje\n","        presion: Presi칩n atmosf칠rica en hPa\n","        viento: Velocidad del viento en km/h\n","\n","    Returns:\n","        An치lisis completo de las condiciones\n","    \"\"\"\n","    analisis = []\n","\n","    # An치lisis de temperatura\n","    if temperatura < 10:\n","        analisis.append(\"Temperatura baja - abrigarse\")\n","    elif temperatura > 30:\n","        analisis.append(\"Temperatura alta - mantenerse hidratado\")\n","    else:\n","        analisis.append(\"Temperatura confortable\")\n","\n","    # An치lisis de humedad\n","    if humedad > 80:\n","        analisis.append(\"Humedad muy alta - sensaci칩n de bochorno\")\n","    elif humedad < 30:\n","        analisis.append(\"Humedad baja - posible sequedad\")\n","\n","    # An치lisis de presi칩n\n","    if presion < 1000:\n","        analisis.append(\"Presi칩n baja - posible mal tiempo\")\n","    elif presion > 1020:\n","        analisis.append(\"Presi칩n alta - tiempo estable esperado\")\n","\n","    # An치lisis de viento\n","    if viento > 40:\n","        analisis.append(\"Viento fuerte - precauci칩n en exteriores\")\n","\n","    return \"AN츼LISIS: \" + \" | \".join(analisis)\n","\n","# 2. Crear conjunto completo de herramientas\n","herramientas_completas = [\n","    convertir_temperatura,\n","    calcular_punto_rocio,\n","    clasificar_indice_uv,\n","    analizar_condiciones_completas\n","]\n","\n","# 3. Crear agente final con memoria\n","from langchain.memory import ConversationBufferMemory\n","\n","memoria_sistema = ConversationBufferMemory(\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")\n","\n","agente_sistema_completo = AgentExecutor(\n","    agent=agente,\n","    tools=herramientas_completas,\n","    memory=memoria_sistema,\n","    verbose=True,\n","    handle_parsing_errors=True,\n",")\n","\n","print(\"Sistema de An치lisis Meteorol칩gico Completo - Inicializado\")"]},{"cell_type":"code","execution_count":null,"id":"2a468ff9","metadata":{"id":"2a468ff9"},"outputs":[],"source":["# 4. Funci칩n helper para interactuar con el sistema\n","def consultar_sistema(pregunta: str, sesion_id: str = \"default\"):\n","    \"\"\"Funci칩n helper para consultar el sistema de forma m치s limpia\"\"\"\n","    resultado = agente_sistema_completo.invoke({\"input\": pregunta})\n","    return resultado[\"output\"]\n","\n","# 5. Probar el sistema completo\n","print(\"\\n\" + \"=\"*70)\n","print(\"DEMO DEL SISTEMA COMPLETO\")\n","print(\"=\"*70)\n","\n","# Caso de uso 1\n","print(\"\\n[Caso 1: An치lisis integral]\")\n","consulta = \"\"\"Tengo las siguientes condiciones:\n","- Temperatura: 26춿C\n","- Humedad: 75%\n","- Presi칩n: 1015 hPa\n","- Viento: 25 km/h\n","- 칈ndice UV: 7\n","\n","Dame un an치lisis completo y recomendaciones.\"\"\"\n","\n","print(f\"Usuario: {consulta}\")\n","print(f\"\\nSistema: {consultar_sistema(consulta, 'sesion_demo_1')}\")\n","\n","# Caso de uso 2 (con memoria)\n","print(\"\\n\\n[Caso 2: Conversaci칩n con contexto]\")\n","consulta2 = \"쮺u치l ser칤a el punto de roc칤o con esos valores de temperatura y humedad?\"\n","print(f\"Usuario: {consulta2}\")\n","print(f\"\\nSistema: {consultar_sistema(consulta2, 'sesion_demo_1')}\")"]},{"cell_type":"markdown","id":"5f91bba4","metadata":{"id":"5f91bba4"},"source":["---\n","\n","## 11. Conceptos Avanzados y Mejores Pr치cticas\n","\n","### 11.1 Manejo de Errores en Herramientas\n","\n","Es importante que las herramientas manejen errores gracefully."]},{"cell_type":"code","execution_count":null,"id":"659be79f","metadata":{"id":"659be79f"},"outputs":[],"source":["@tool\n","def herramienta_robusta(valor: float) -> str:\n","    \"\"\"Ejemplo de herramienta con manejo de errores.\n","\n","    Args:\n","        valor: Un valor num칠rico para procesar\n","\n","    Returns:\n","        Resultado del procesamiento\n","    \"\"\"\n","    try:\n","        if valor < 0:\n","            return \"ERROR: El valor debe ser positivo\"\n","\n","        if valor > 100:\n","            return \"ADVERTENCIA: Valor inusualmente alto, pero procesando...\"\n","\n","        resultado = valor * 2\n","        return f\"Procesamiento exitoso: {resultado}\"\n","\n","    except Exception as e:\n","        return f\"ERROR inesperado: {str(e)}\"\n","\n","# Probar\n","print(herramienta_robusta.invoke({\"valor\": 50}))\n","print(herramienta_robusta.invoke({\"valor\": -10}))\n","print(herramienta_robusta.invoke({\"valor\": 150}))"]},{"cell_type":"markdown","id":"62e8a947","metadata":{"id":"62e8a947"},"source":["### 11.2 Optimizaci칩n de Prompts\n","\n","Consejos para crear prompts efectivos:\n","\n","1. **Sea espec칤fico**: Define claramente el rol y comportamiento esperado\n","2. **Use ejemplos**: Few-shot learning mejora significativamente los resultados\n","3. **Estructure la salida**: Use parsers para obtener datos estructurados\n","4. **Itere y pruebe**: Los prompts mejoran con experimentaci칩n\n","5. **Considere el contexto**: Incluya informaci칩n relevante pero concisa\n","\n","### 11.3 Mejores Pr치cticas para Agentes\n","\n","1. **Herramientas at칩micas**: Cada herramienta debe hacer una cosa bien\n","2. **Descripciones claras**: Los docstrings son cr칤ticos para que el agente entienda cu치ndo usar cada herramienta\n","3. **Validaci칩n de entrada**: Siempre valide los par치metros de las herramientas\n","4. **Manejo de errores**: Retorne mensajes de error descriptivos\n","5. **Testing**: Pruebe cada herramienta independientemente antes de integrarla"]},{"cell_type":"markdown","id":"bb20dd1d","metadata":{"id":"bb20dd1d"},"source":["---\n","\n","## 12. Conclusiones y Pr칩ximos Pasos\n","\n","### Lo que has aprendido\n","\n","En este taller has dominado:\n","\n","1. **Fundamentos de LangChain**: Arquitectura y componentes principales\n","2. **Mensajes y Prompts**: Diferentes tipos y c칩mo usarlos efectivamente\n","3. **Plantillas**: PromptTemplate, ChatPromptTemplate, y FewShotChatMessagePromptTemplate\n","4. **Cadenas**: Encadenamiento de operaciones con LCEL\n","5. **Output Parsers**: Estructuraci칩n de respuestas del modelo\n","6. **Memoria**: Manejo de contexto conversacional\n","7. **Serializaci칩n**: Guardar y cargar prompts\n","8. **Herramientas**: Creaci칩n y uso de herramientas personalizadas\n","9. **Agentes**: Toma de decisiones aut칩nomas\n","10. **Integraci칩n**: Construcci칩n de sistemas completos\n","\n","### Aplicaciones Reales\n","\n","Estos conceptos se aplican en:\n","\n","- Chatbots empresariales con acceso a bases de datos\n","- Asistentes de an치lisis de datos\n","- Sistemas de automatizaci칩n de tareas\n","- Herramientas de investigaci칩n y s칤ntesis de informaci칩n\n","- Interfaces conversacionales para APIs\n","- Sistemas de recomendaci칩n inteligentes\n","\n","### Recursos para Continuar\n","\n","- Documentaci칩n oficial de LangChain: https://python.langchain.com\n","- LangSmith para debugging y observabilidad: https://smith.langchain.com\n","- Comunidad de LangChain en Discord\n","- Repositorio de LangChain en GitHub\n","\n","### Desaf칤o Final\n","\n","Construye un agente que:\n","1. Use al menos 4 herramientas personalizadas\n","2. Integre memoria conversacional\n","3. Maneje diferentes tipos de consultas\n","4. Procese salidas estructuradas con parsers\n","5. Incluya manejo de errores robusto\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}